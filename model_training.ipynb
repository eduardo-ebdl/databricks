{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d2ccb9-b129-47e5-b95e-230631db4920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SETUP - IMPORTING LIBRARIES\n",
    "# ==============================================================================\n",
    "# This section imports all the necessary libraries for data manipulation,\n",
    "# machine learning, and data visualization.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.tree import plot_tree\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This command ensures that plots are displayed directly in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "# ==============================================================================\n",
    "# DATA LOADING AND PREPARATION\n",
    "# ==============================================================================\n",
    "# Here, we load the patient symptom data from the intermediate table created\n",
    "# in the previous SQL step. The data is then converted into a Pandas DataFrame.\n",
    "\n",
    "# SQL query to select all data from the prepared table.\n",
    "patients_query = \"\"\"\n",
    "SELECT * FROM dev_db.intermediate.int_patients_symptoms\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and load data into a Spark DataFrame.\n",
    "# Note: This assumes a Spark environment is available.\n",
    "df_spark = spark.sql(patients_query)\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame for use with scikit-learn.\n",
    "df_pandas = df_spark.toPandas()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "# The model needs numerical input. This section converts the 'Yes'/'No' values\n",
    "# in the symptom columns into 1s and 0s.\n",
    "\n",
    "# List of columns that contain symptom information.\n",
    "symptom_columns = ['fever', 'cough', 'runny_nose', 'headache', 'body_pain', 'fatigue', 'nausea', 'diarrhea']\n",
    "\n",
    "# Loop through each symptom column and map 'Yes' to 1 and 'No' to 0.\n",
    "for col in symptom_columns:\n",
    "    df_pandas[col] = df_pandas[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Separate the features (symptoms, X) from the target variable (diagnose, y).\n",
    "X = df_pandas[symptom_columns] # Features (what we use to predict)\n",
    "y = df_pandas['diagnose']      # Target (what we want to predict)\n",
    "\n",
    "# Display the first few rows of the features to verify the transformation.\n",
    "print(\"Transformed Feature Data (X):\")\n",
    "display(X.head())\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODEL TRAINING AND PREDICTION\n",
    "# ==============================================================================\n",
    "# The data is split into training and testing sets.\n",
    "# A Random Forest Classifier model is then trained on the training data.\n",
    "\n",
    "# Split the dataset: 80% for training and 20% for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model.\n",
    "# n_estimators=100 means the model will build 100 decision trees.\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model using the training data.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODEL EVALUATION\n",
    "# ==============================================================================\n",
    "# We evaluate the model's performance using accuracy and a classification report.\n",
    "# A confusion matrix is also generated to visualize performance.\n",
    "\n",
    "# Calculate the accuracy of the model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy:.2f}\\n')\n",
    "\n",
    "# Print a detailed classification report.\n",
    "# This shows precision, recall, and f1-score for each diagnosis class.\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Create and display the confusion matrix.\n",
    "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Diagnosis')\n",
    "plt.ylabel('Actual Diagnosis')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# ==============================================================================\n",
    "# This visualization shows which symptoms (features) were most important\n",
    "# in making the predictions.\n",
    "\n",
    "# Get feature importances from the trained model.\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Plot the feature importances.\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feature_importances, y=feature_importances.index)\n",
    "plt.title(\"Symptom Importance in Diagnosis\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Symptom\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# DECISION TREE VISUALIZATION\n",
    "# ==============================================================================\n",
    "# To understand how the model makes decisions, we can visualize one of the\n",
    "# individual decision trees from the Random Forest.\n",
    "\n",
    "plt.figure(figsize=(25, 20))\n",
    "\n",
    "# Plot the first tree (estimator) in the forest.\n",
    "plot_tree(model.estimators_[0],\n",
    "          feature_names=X.columns.tolist(),\n",
    "          class_names=model.classes_.tolist(),\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "\n",
    "plt.title(\"Example Decision Tree from the Random Forest Model\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
